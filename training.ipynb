{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec5e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import skimage.io\n",
    "\n",
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4007117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4937011e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ecc89",
   "metadata": {},
   "source": [
    "**Load in data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc722e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>actor</th>\n",
       "      <th>emotions</th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>statement</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/speech-emotion-recognition-ravdess-data/A...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>data/images/03-01-01-01-01-01-01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data/speech-emotion-recognition-ravdess-data/A...</td>\n",
       "      <td>1</td>\n",
       "      <td>calm</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data/images/03-01-02-01-01-01-01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>data/speech-emotion-recognition-ravdess-data/A...</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>data/images/03-01-03-01-01-01-01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>data/speech-emotion-recognition-ravdess-data/A...</td>\n",
       "      <td>1</td>\n",
       "      <td>sad</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>data/images/03-01-04-01-01-01-01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>data/speech-emotion-recognition-ravdess-data/A...</td>\n",
       "      <td>1</td>\n",
       "      <td>angry</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>data/images/03-01-05-01-01-01-01.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          file_name  actor  \\\n",
       "0           0  data/speech-emotion-recognition-ravdess-data/A...      1   \n",
       "1           1  data/speech-emotion-recognition-ravdess-data/A...      1   \n",
       "2           2  data/speech-emotion-recognition-ravdess-data/A...      1   \n",
       "3           3  data/speech-emotion-recognition-ravdess-data/A...      1   \n",
       "4           4  data/speech-emotion-recognition-ravdess-data/A...      1   \n",
       "\n",
       "  emotions  emotion_id  statement                            image_path  \n",
       "0  neutral           1          1  data/images/03-01-01-01-01-01-01.png  \n",
       "1     calm           2          1  data/images/03-01-02-01-01-01-01.png  \n",
       "2    happy           3          1  data/images/03-01-03-01-01-01-01.png  \n",
       "3      sad           4          1  data/images/03-01-04-01-01-01-01.png  \n",
       "4    angry           5          1  data/images/03-01-05-01-01-01-01.png  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/RAVDESS_processed_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19228ee",
   "metadata": {},
   "source": [
    "**Dataset and Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4534ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EmotionDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fce7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92e88c",
   "metadata": {},
   "source": [
    "**Initial Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0efefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(ngpus=1)\n",
    "disc = Discriminator(ngpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929cfc2",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36bb87dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "full() received an invalid combination of arguments - got (tuple, str, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f1f60788e88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Summer_Module/GANs-for-Speech-Emotion-Recognition-and-Translation/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader, generator_model, discriminator_model, num_epochs, ep)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mreal_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_cpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: full() received an invalid combination of arguments - got (tuple, str, device=torch.device, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "train(train_dl, gen, disc, num_epochs=10, ep=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdabda",
   "metadata": {},
   "source": [
    "**Model Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eef29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "013aea75",
   "metadata": {},
   "source": [
    "**Model Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d951de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
